{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushKataria/AIPND/blob/master/Copy_of_HostLlama2BehindAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hosting Llama 2 with Free GPU via Google Collab**"
      ],
      "metadata": {
        "id": "bznyRLfLg750"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Before getting started, if running on Google Colab, check that the runtime is set to T4 GPU**"
      ],
      "metadata": {
        "id": "HRDd_-CFIqRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies\n",
        "- Requirements for running FastAPI Server\n",
        "- Requirements for creating a public model serving URL via Ngrok\n",
        "- Requirements for running Llama2 13B (including Quantization)\n"
      ],
      "metadata": {
        "id": "kU0GYiR4hWKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "gDVaaatLEpzq",
        "outputId": "b40d9c9b-a37d-445b-a5ce-81485c6f2fb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.1.83)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.7.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u24fGvOmT2Pi",
        "outputId": "634c1375-ca6a-4039-9a82-d178f7f07a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi[all]\n",
            "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi[all])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (4.7.1)\n",
            "Collecting email-validator>=2.0.0 (from fastapi[all])\n",
            "  Downloading email_validator-2.0.0.post2-py3-none-any.whl (31 kB)\n",
            "Collecting httpx>=0.23.0 (from fastapi[all])\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (2.1.2)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (3.1.2)\n",
            "Collecting orjson>=3.2.1 (from fastapi[all])\n",
            "  Downloading orjson-3.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic-extra-types>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_extra_types-2.1.0-py3-none-any.whl (16 kB)\n",
            "Collecting pydantic-settings>=2.0.0 (from fastapi[all])\n",
            "  Downloading pydantic_settings-2.0.3-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (6.0.1)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi[all])\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.3)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.3 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.14)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi[all]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi[all]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi[all]) (1.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
            "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->fastapi[all])\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi[all]) (2023.7.22)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx>=0.23.0->fastapi[all])\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi[all]) (2.1.3)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings>=2.0.0->fastapi[all])\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Collecting httptools>=0.5.0 (from uvicorn)\n",
            "  Downloading httptools-0.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.8/428.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn)\n",
            "  Downloading uvloop-0.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn)\n",
            "  Downloading watchfiles-0.20.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Installing collected packages: tokenizers, safetensors, websockets, uvloop, ujson, python-multipart, python-dotenv, orjson, httptools, h11, dnspython, watchfiles, uvicorn, starlette, huggingface-hub, httpcore, email-validator, transformers, pydantic-settings, pydantic-extra-types, httpx, fastapi\n",
            "Successfully installed dnspython-2.4.2 email-validator-2.0.0.post2 fastapi-0.103.1 h11-0.14.0 httpcore-0.17.3 httptools-0.6.0 httpx-0.24.1 huggingface-hub-0.16.4 orjson-3.9.6 pydantic-extra-types-2.1.0 pydantic-settings-2.0.3 python-dotenv-1.0.0 python-multipart-0.0.6 safetensors-0.3.3 starlette-0.27.0 tokenizers-0.13.3 transformers-4.33.1 ujson-5.8.0 uvicorn-0.23.2 uvloop-0.17.0 watchfiles-0.20.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n",
        "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "metadata": {
        "id": "Kxx3xpr2cXHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ff1bd03-cd60-4ccd-aeb7-d23ebdc7d945"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-08 03:53:31--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 18.205.222.128, 52.202.168.65, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  17.3MB/s    in 0.8s    \n",
            "\n",
            "2023-09-08 03:53:32 (17.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ngrok is used to make the FastAPI server accessible via a public URL.\n",
        "\n",
        "Users are required to make a free account and provide their auth token to use Ngrok. The free version only allows 1 local tunnel and the auth token is used to track this usage limit."
      ],
      "metadata": {
        "id": "WPOYhvtAHiW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!./ngrok authtoken 2UuzNGiRLtKJnda23tfjaEjf9f8_23UhninTFZ8jjHb2jSxy4"
      ],
      "metadata": {
        "id": "Nw3LWCfNcg9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a500c108-20ba-401e-bc6b-3d8d7c54daf8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Json Grammar for response\n",
        "%%writefile grammar_json.txt\n",
        "type Response = {\n",
        "  \"final_answer\": string,\n",
        "  \"sources\": [string, string, string, string],\n",
        "  \"related_sources\": [string, string, string, string]\n",
        "};"
      ],
      "metadata": {
        "id": "9Y9-oVQW8tnW",
        "outputId": "3e0912d8-447e-4ace-8572-581f2ee96496",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting grammar_json.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a little parser combinator microlibrary for help parsing the grammars\n",
        "%%writefile grammar.py\n",
        "def none_of(chars):\n",
        "    def inner(s):\n",
        "        if s[0] in chars:\n",
        "            raise ValueError(\"unexpected \" + s[0])\n",
        "        return (s[1:], s[0])\n",
        "    return inner\n",
        "\n",
        "def one_of(chars):\n",
        "    def inner(s):\n",
        "        if not s or s[0] not in chars:\n",
        "            raise ValueError(\"expected one of \" + chars)\n",
        "        return (s[1:], s[0])\n",
        "    return inner\n",
        "\n",
        "def series(*parsers):\n",
        "    def inner(s):\n",
        "        result = []\n",
        "        for parser in parsers:\n",
        "            (s, value) = parser(s)\n",
        "            result.append(value)\n",
        "        return (s, result)\n",
        "    return inner\n",
        "\n",
        "def alt(*parsers):\n",
        "    def inner(s):\n",
        "        exceptions = []\n",
        "        for parser in parsers:\n",
        "            try:\n",
        "                return parser(s)\n",
        "            except ValueError as e:\n",
        "                exceptions.append(e)\n",
        "        raise ValueError(\"expected one of \" + \", \".join(map(str, exceptions)) + \" but got \" + s[:5])\n",
        "    return inner\n",
        "\n",
        "def many(parser):\n",
        "    def inner(s):\n",
        "        result = []\n",
        "        while True:\n",
        "            try:\n",
        "                (s, value) = parser(s)\n",
        "                result.append(value)\n",
        "            except ValueError:\n",
        "                break\n",
        "        return (s, result)\n",
        "    return inner\n",
        "\n",
        "def maybe(parser):\n",
        "    def inner(s):\n",
        "        try:\n",
        "            (s, value) = parser(s)\n",
        "            return (s, value)\n",
        "        except ValueError:\n",
        "            return (s, None)\n",
        "    return inner\n",
        "\n",
        "def many1(parser):\n",
        "    def inner(s):\n",
        "        (s, value) = parser(s)\n",
        "        (s, values) = many(parser)(s)\n",
        "        return (s, [value] + values)\n",
        "    return inner\n",
        "\n",
        "def intersperse(parser, sep_parser):\n",
        "    def inner(s):\n",
        "        try:\n",
        "            (s, value) = parser(s)\n",
        "            (s, values) = many(series(sep_parser, parser))(s)\n",
        "            return (s, [value] + [v for (_, v) in values])\n",
        "        except ValueError:\n",
        "            return (s, [])\n",
        "    return inner\n",
        "\n",
        "def span_spaces():\n",
        "    def inner(s):\n",
        "        (s, _) = many(one_of(\" \\t\"))(s)\n",
        "        return (s, None)\n",
        "    return inner\n",
        "\n",
        "def spaces():\n",
        "    def inner(s):\n",
        "        (s, _) = many(one_of(\" \\t\\r\\n\"))(s)\n",
        "        return (s, None)\n",
        "    return inner\n",
        "\n",
        "def nl():\n",
        "    return alt(literal(\"\\r\\n\"), literal(\"\\n\"))\n",
        "\n",
        "def alpha():\n",
        "    return one_of(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
        "\n",
        "def digit():\n",
        "    return one_of(\"0123456789\")\n",
        "\n",
        "def literal(string):\n",
        "    return series(*[one_of(c) for c in string])\n",
        "\n",
        "# Grammar data structures\n",
        "\n",
        "class Terminal:\n",
        "    def __init__(self, value):\n",
        "        if not isinstance(value, str):\n",
        "            raise TypeError(\"Terminal value must be a string\")\n",
        "        self.bytes = [ord(c) for c in value]\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.bytes == other.bytes\n",
        "\n",
        "class NonTerminal:\n",
        "    def __init__(self, rule_name):\n",
        "        self.rule_name = rule_name\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.rule_name == other.rule_name\n",
        "\n",
        "class Branch:\n",
        "    def __init__(self, elements):\n",
        "        if not isinstance(elements, list):\n",
        "            raise TypeError(\"Branch elements must be a list\")\n",
        "        if not all(isinstance(element, (Terminal, NonTerminal)) for element in elements):\n",
        "            raise TypeError(\"Branch elements must be a list of Terminals and NonTerminals\")\n",
        "        self.elements = elements\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.elements == other.elements\n",
        "\n",
        "class Rule:\n",
        "    def __init__(self, rule_name, branches):\n",
        "        self.rule_name = rule_name\n",
        "        self.branches = branches\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.rule_name == other.rule_name and self.branches == other.branches\n",
        "\n",
        "class Grammar:\n",
        "    def __init__(self, rules, main_rule):\n",
        "        self.rules = rules\n",
        "        self.main_rule = main_rule\n",
        "\n",
        "    def add_rule(self, rule):\n",
        "        if isinstance(rule, str):\n",
        "            (rem, rule) = parse_rule()(rule)\n",
        "            if rem:\n",
        "                raise ValueError(\"rule had trailing characters: \" + rem)\n",
        "\n",
        "        if rule.rule_name in self.rules:\n",
        "            if rule != self.rules[rule.rule_name]:\n",
        "                raise ValueError(\"rule name collision\")\n",
        "        else:\n",
        "            self.rules[rule.rule_name] = rule\n",
        "\n",
        "    def grammar(self):\n",
        "        def get_rule_names(node, visited=None):\n",
        "            if visited is None:\n",
        "                visited = set()\n",
        "            if isinstance(node, NonTerminal):\n",
        "                return get_rule_names(self.rules[node.rule_name], visited)\n",
        "            elif isinstance(node, Branch):\n",
        "                for element in node.elements:\n",
        "                    get_rule_names(element, visited)\n",
        "            elif isinstance(node, Rule):\n",
        "                if node.rule_name not in visited:\n",
        "                    visited.add(node.rule_name)\n",
        "                    for branch in node.branches:\n",
        "                        get_rule_names(branch, visited)\n",
        "            return visited\n",
        "\n",
        "        all_rules = get_rule_names(self.main_rule)\n",
        "        rule_ids = { rule_name : i for (i, rule_name) in enumerate(all_rules) }\n",
        "        id = lambda rule: rule_ids[rule.rule_name]\n",
        "\n",
        "        result = \"\"\n",
        "        result += f\"{id(self.main_rule)} {len(all_rules)}\\n\"\n",
        "        for rule_name in rule_ids:\n",
        "            rule = self.rules[rule_name]\n",
        "            result += f\"{id(rule)} {len(rule.branches)}\\n\"\n",
        "            for branch in rule.branches:\n",
        "                result += f\"{len(branch.elements)}\\n\"\n",
        "                for element in branch.elements:\n",
        "                    if isinstance(element, Terminal):\n",
        "                        result += f\"0 {len(element.bytes)} {' '.join(map(str, element.bytes))}\\n\"\n",
        "                    elif isinstance(element, NonTerminal):\n",
        "                        result += f\"1 {id(element)}\\n\"\n",
        "                    else:\n",
        "                        raise TypeError(\"unsupported element type\")\n",
        "\n",
        "        return result\n",
        "\n",
        "# Grammar parsing\n",
        "\n",
        "def rule_char():\n",
        "    return alt(alpha(), digit(), one_of(\"_-.\"))\n",
        "\n",
        "def parse_rule_name():\n",
        "    def inner(s):\n",
        "        (s, chars) = many1(rule_char())(s)\n",
        "        return (s, \"\".join(chars))\n",
        "    return inner\n",
        "\n",
        "def parse_terminal():\n",
        "    def double_quoted(s):\n",
        "        (s, _) = literal(\"\\\"\")(s)\n",
        "        (s, chars) = many(alt(none_of(\"\\\"\"), literal(\"\\\\\\\"\")))(s)\n",
        "        (s, _) = literal(\"\\\"\")(s)\n",
        "        return (s, Terminal(\"\".join(chars)))\n",
        "\n",
        "    def single_quoted(s):\n",
        "        (s, _) = literal(\"'\")(s)\n",
        "        (s, chars) = many(alt(none_of(\"'\"), literal(\"\\\\'\")))(s)\n",
        "        (s, _) = literal(\"'\")(s)\n",
        "        return (s, Terminal(\"\".join(chars)))\n",
        "\n",
        "    return alt(double_quoted, single_quoted)\n",
        "\n",
        "def parse_non_terminal():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"<\")(s)\n",
        "        (s, rule_name) = parse_rule_name()(s)\n",
        "        (s, _) = literal(\">\")(s)\n",
        "        return (s, NonTerminal(rule_name))\n",
        "    return inner\n",
        "\n",
        "def parse_element():\n",
        "    return alt(parse_terminal(), parse_non_terminal())\n",
        "\n",
        "def parse_branch():\n",
        "    def inner(s):\n",
        "        (s, elements) = intersperse(parse_element(), span_spaces())(s)\n",
        "        return (s, Branch(elements))\n",
        "    return inner\n",
        "\n",
        "def parse_rule_body():\n",
        "    return intersperse(parse_branch(), series(span_spaces(), literal(\"|\"), span_spaces()))\n",
        "\n",
        "def parse_rule():\n",
        "    def inner(s):\n",
        "        (s, rule_name) = parse_rule_name()(s)\n",
        "        (s, _) = span_spaces()(s)\n",
        "        (s, _) = literal(\"=\")(s)\n",
        "        (s, _) = span_spaces()(s)\n",
        "        (s, branches) = parse_rule_body()(s)\n",
        "        (s, _) = span_spaces()(s)\n",
        "        return (s, Rule(rule_name, branches))\n",
        "    return inner\n",
        "\n",
        "def grammar():\n",
        "    def inner(s):\n",
        "        (s, rules) = intersperse(parse_rule(), nl())(s)\n",
        "        (s, _) = spaces()(s)\n",
        "        return (s, rules)\n",
        "    return inner\n",
        "\n",
        "def type_char():\n",
        "    return alt(alpha(), digit(), one_of(\"_\"))\n",
        "\n",
        "def parse_type_name():\n",
        "    def inner(s):\n",
        "        (s, chars) = many1(type_char())(s)\n",
        "        return (s, \"\".join(chars))\n",
        "    return inner\n",
        "\n",
        "# JSON spec data structures\n",
        "\n",
        "class JsonType:\n",
        "    def __eq__(self, other):\n",
        "        return repr(self) == repr(other)\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(repr(self))\n",
        "\n",
        "    def rule_name(self):\n",
        "        return f\"rule_{abs(hash(repr(self)))}\"\n",
        "\n",
        "    def expr_rule_name(self):\n",
        "        return f\"<{self.rule_name()}>\"\n",
        "\n",
        "    def visit_types(self):\n",
        "        yield self\n",
        "\n",
        "class JsonBoolean(JsonType):\n",
        "    def __repr__(self):\n",
        "        return \"boolean\"\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = \"true\" | \"false\"')\n",
        "\n",
        "class JsonInteger(JsonType):\n",
        "    def __repr__(self):\n",
        "        return \"integer\"\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = <sign> <digits>')\n",
        "\n",
        "class JsonUnsigned(JsonType):\n",
        "    def __repr__(self):\n",
        "        return \"unsigned\"\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = <digits>')\n",
        "\n",
        "class JsonFloat(JsonType):\n",
        "    def __repr__(self):\n",
        "        return \"float\"\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = <sign> <digits> | <sign> <digits> \".\" <digits>')\n",
        "\n",
        "class JsonString(JsonType):\n",
        "    def __repr__(self):\n",
        "        return \"string\"\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = ' + '''   '\"' <string_chars> '\"'    '''.strip())\n",
        "\n",
        "class JsonNull(JsonType):\n",
        "    def __repr__(self):\n",
        "        return \"null\"\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = \"null\"')\n",
        "\n",
        "class JsonTuple(JsonType):\n",
        "    def __init__(self, types):\n",
        "        self.types = types\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"[\" + \", \".join(map(repr, self.types)) + \"]\"\n",
        "\n",
        "    def visit_types(self):\n",
        "        yield from super().visit_types()\n",
        "        for t in self.types:\n",
        "            yield from t.visit_types()\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        for t in self.types:\n",
        "            t.add_to_grammar(grammar, user_types)\n",
        "\n",
        "        rule_body = ' \"[\" '\n",
        "        for (i, t) in enumerate(self.types):\n",
        "            if i > 0:\n",
        "                rule_body += ' \", \" '\n",
        "            rule_body += f'{self.types[i].expr_rule_name()}'\n",
        "        rule_body += ' \"]\" '\n",
        "\n",
        "        grammar.add_rule(f'{self.rule_name()} = {rule_body}')\n",
        "\n",
        "class JsonObject(JsonType):\n",
        "    def __init__(self, fields):\n",
        "        self.fields = fields\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"{\" + \", \".join(map(lambda f: f\"{f[0]}: {f[1]}\", self.fields)) + \"}\"\n",
        "\n",
        "    def visit_types(self):\n",
        "        yield from super().visit_types()\n",
        "        for (_, t) in self.fields:\n",
        "            yield from t.visit_types()\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        for (_, t) in self.fields:\n",
        "            t.add_to_grammar(grammar, user_types)\n",
        "\n",
        "        rule_body = ' \"{\" '\n",
        "        for (i, (k, t)) in enumerate(self.fields):\n",
        "            if i > 0:\n",
        "                rule_body += ' \",\" '\n",
        "            rule_body += f\"' \\\"{k}\\\"'\"\n",
        "            rule_body += f' \": \" {t.expr_rule_name()} \" \"'\n",
        "        rule_body += ' \"}\" '\n",
        "\n",
        "        grammar.add_rule(f'{self.rule_name()} = {rule_body}')\n",
        "\n",
        "class JsonUnion(JsonType):\n",
        "    def __init__(self, types):\n",
        "        types = { repr(t) : t for t in types}\n",
        "        self.types = sorted(list(types.values()), key=repr)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \" | \".join(map(repr, self.types))\n",
        "\n",
        "    def visit_types(self):\n",
        "        yield from super().visit_types()\n",
        "        for t in self.types:\n",
        "            yield from t.visit_types()\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        for t in self.types:\n",
        "            t.add_to_grammar(grammar, user_types)\n",
        "\n",
        "        rule_body = ''\n",
        "        for (i, t) in enumerate(self.types):\n",
        "            if i > 0:\n",
        "                rule_body += ' | '\n",
        "            rule_body += f'{t.expr_rule_name()}'\n",
        "\n",
        "        grammar.add_rule(f'{self.rule_name()} = {rule_body}')\n",
        "\n",
        "class JsonArray(JsonType):\n",
        "    def __init__(self, value_type):\n",
        "        self.value_type = value_type\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Array<{self.value_type}>\"\n",
        "\n",
        "    def visit_types(self):\n",
        "        yield from super().visit_types()\n",
        "        yield from self.value_type.visit_types()\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        self.value_type.add_to_grammar(grammar, user_types)\n",
        "        value_rule_name = self.value_type.rule_name()\n",
        "        value_expr_rule_name = self.value_type.rule_name()\n",
        "        many_rule_name = f'{value_rule_name}_many'\n",
        "        many_expr_rule_name = f'<{many_rule_name}>'\n",
        "        grammar.add_rule(f'{many_rule_name} = {value_expr_rule_name} | {value_expr_rule_name} \", \" {many_expr_rule_name}')\n",
        "        grammar.add_rule(f'{self.rule_name()} = \"[\" {many_expr_rule_name} \"]\"')\n",
        "\n",
        "class JsonUserType(JsonType):\n",
        "    def __init__(self, type_name):\n",
        "        self.type_name = type_name\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.type_name\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        user_type = user_types[self.type_name]\n",
        "        user_type.add_to_grammar(grammar, user_types)\n",
        "        grammar.add_rule(f'{self.rule_name()} = {user_type.expr_rule_name()}')\n",
        "\n",
        "class JsonStringLiteralType(JsonType):\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'\"{self.value}\"'\n",
        "\n",
        "    def add_to_grammar(self, grammar, user_types):\n",
        "        grammar.add_rule(f'{self.rule_name()} = \"{self.value}\"')\n",
        "\n",
        "class JsonSpec:\n",
        "    def __init__(self, types):\n",
        "        self.types = types\n",
        "\n",
        "    def grammar(self):\n",
        "        g = Grammar({}, None)\n",
        "        g.add_rule('nil = ')\n",
        "        g.add_rule('digit = \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\"')\n",
        "        g.add_rule('digits = <digit> | <digit> <digits>')\n",
        "        g.add_rule('sign = \"-\" | <nil>')\n",
        "        g.add_rule('alphanumspace = ' + \" | \".join(map(lambda c: f'\"{c}\"', \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 \")))\n",
        "        g.add_rule('escaped_double_quote = \\'\\\\\"\\'')\n",
        "        g.add_rule('string_char = <alphanumspace> | <escaped_double_quote>')\n",
        "        g.add_rule('string_chars = <nil> | <string_char> <string_chars>')\n",
        "\n",
        "        types_by_name = { n : t for (n, t) in self.types }\n",
        "\n",
        "        for (_, t) in self.types:\n",
        "            t.add_to_grammar(g, types_by_name)\n",
        "\n",
        "        main_type = self.types[-1][1]\n",
        "        main_type_name = main_type.rule_name()\n",
        "\n",
        "        g.main_rule = g.rules[main_type_name]\n",
        "\n",
        "        return g.grammar()\n",
        "\n",
        "# JSON spec parsing\n",
        "\n",
        "def parse_boolean_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"boolean\")(s)\n",
        "        return (s, JsonBoolean())\n",
        "    return inner\n",
        "\n",
        "def parse_integer_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"integer\")(s)\n",
        "        return (s, JsonInteger())\n",
        "    return inner\n",
        "\n",
        "def parse_unsigned_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"unsigned\")(s)\n",
        "        return (s, JsonUnsigned())\n",
        "    return inner\n",
        "\n",
        "def parse_float_type():\n",
        "    def inner(s):\n",
        "        (s, _) = alt(literal(\"float\"), literal(\"double\"), literal(\"number\"))(s)\n",
        "        return (s, JsonFloat())\n",
        "    return inner\n",
        "\n",
        "def parse_string_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"string\")(s)\n",
        "        return (s, JsonString())\n",
        "    return inner\n",
        "\n",
        "def parse_null_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"null\")(s)\n",
        "        return (s, JsonNull())\n",
        "    return inner\n",
        "\n",
        "def parse_tuple_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"[\")(s)\n",
        "        (s, types) = intersperse(parse_type_body(), series(spaces(), literal(\",\"), spaces()))(s)\n",
        "        (s, _) = series(maybe(literal(\",\")), spaces())(s)\n",
        "        (s, _) = literal(\"]\")(s)\n",
        "        return (s, JsonTuple(types))\n",
        "    return inner\n",
        "\n",
        "def parse_object_key():\n",
        "    def inner(s):\n",
        "        (s, _) = literal('\"')(s)\n",
        "        (s, key) = many1(alt(none_of('\"'), literal('\\\\\"')))(s)\n",
        "        (s, _) = literal('\"')(s)\n",
        "        return (s, \"\".join(key))\n",
        "    return inner\n",
        "\n",
        "def parse_object_field():\n",
        "    def inner(s):\n",
        "        (s, key) = parse_object_key()(s)\n",
        "        (s, _) = series(span_spaces(), literal(\":\"), span_spaces())(s)\n",
        "        (s, value) = parse_type_body()(s)\n",
        "        return (s, (key, value))\n",
        "    return inner\n",
        "\n",
        "def parse_object_type():\n",
        "    def inner(s):\n",
        "        (s, _) = series(literal(\"{\"), spaces())(s)\n",
        "        (s, fields) = intersperse(parse_object_field(), series(spaces(), literal(\",\"), spaces()))(s)\n",
        "        (s, _) = series(maybe(literal(\",\")), spaces(), literal(\"}\"))(s)\n",
        "        return (s, JsonObject(fields))\n",
        "    return inner\n",
        "\n",
        "def parse_array_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal(\"Array<\")(s)\n",
        "        (s, value_type) = parse_type_body()(s)\n",
        "        (s, _) = literal(\">\")(s)\n",
        "        return (s, JsonArray(value_type))\n",
        "    return inner\n",
        "\n",
        "def parse_user_type():\n",
        "    def inner(s):\n",
        "        (s, type_name) = parse_type_name()(s)\n",
        "        return (s, JsonUserType(type_name))\n",
        "    return inner\n",
        "\n",
        "def parse_string_literal_type():\n",
        "    def inner(s):\n",
        "        (s, _) = literal('\"')(s)\n",
        "        (s, value) = many1(alt(none_of('\"'), literal('\\\\\"')))(s)\n",
        "        (s, _) = literal('\"')(s)\n",
        "        return (s, JsonStringLiteralType(\"\".join(value)))\n",
        "    return inner\n",
        "\n",
        "def parse_single_type_body():\n",
        "    return alt(\n",
        "        parse_boolean_type(),\n",
        "        parse_integer_type(),\n",
        "        parse_unsigned_type(),\n",
        "        parse_null_type(),\n",
        "        parse_float_type(),\n",
        "        parse_string_type(),\n",
        "        parse_tuple_type(),\n",
        "        parse_object_type(),\n",
        "        parse_array_type(),\n",
        "        parse_string_literal_type(),\n",
        "        parse_user_type(),\n",
        "    )\n",
        "\n",
        "def parse_type_body():\n",
        "    def inner(s):\n",
        "        (s, types) = intersperse(parse_single_type_body(), series(span_spaces(), literal(\"|\"), span_spaces()))(s)\n",
        "        if len(types) == 1:\n",
        "            return (s, types[0])\n",
        "        else:\n",
        "            return (s, JsonUnion(types))\n",
        "    return inner\n",
        "\n",
        "def parse_type_definition():\n",
        "    def inner(s):\n",
        "        (s, _) = series(spaces(), literal(\"type \"))(s)\n",
        "        (s, type_name) = parse_type_name()(s)\n",
        "        (s, _) = series(spaces(), literal(\"=\"), spaces())(s)\n",
        "        (s, type_body) = parse_type_body()(s)\n",
        "        (s, _) = series(spaces(), literal(\";\"))(s)\n",
        "        return (s, (type_name, type_body))\n",
        "    return inner\n",
        "\n",
        "def json_spec(s):\n",
        "    (s, types) = many1(parse_type_definition())(s)\n",
        "    (s, _) = spaces()(s)\n",
        "    if s:\n",
        "        raise Exception(\"JSON spec had unparsed trailing characters:\\n\" + s)\n",
        "    return JsonSpec(types)\n",
        "\n",
        "HELP = \"\"\"\n",
        "\n",
        "--grammar <path> : parse a full grammar from a file\n",
        "--grammar <string> : parse a full grammar from a string\n",
        "\n",
        "The last rule in the grammar is the main rule.\n",
        "\n",
        "Example grammar:\n",
        "person = \"Bob\" | \"Alice\" | \"Charlie\"\n",
        "digit = \"0\" | \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\"\n",
        "age = digit | digit digit\n",
        "sentence = person \"is\" age \"years old\"\n",
        "\n",
        "--json <path> : parse a json spec from a file\n",
        "--json <string> : parse a json spec from a string\n",
        "\n",
        "The last type declaration in the json spec is the main type.\n",
        "\n",
        "Example JSON spec:\n",
        "type FirstNameLastName = [string, string];\n",
        "type Result = {\n",
        "    \"country\": string,\n",
        "    \"population\": integer,\n",
        "    \"percent_retired\": float,\n",
        "    \"cities\": Array<{ \"city\": string, \"is_capital\": boolean }>,\n",
        "    \"head_of_state\": FirstNameLastName | null,\n",
        "};\n",
        "\n",
        "\"\"\".strip()\n",
        "\n",
        "def main():\n",
        "    import sys\n",
        "    # modes: parse a full grammar, parse a json spec\n",
        "    if len(sys.argv) < 3 or sys.argv[1] in [\"--help\", \"-h\", \"help\"]:\n",
        "        print(HELP)\n",
        "        sys.exit(1)\n",
        "\n",
        "    if sys.argv[1] == \"--grammar\":\n",
        "        grammar_string = None\n",
        "        # try to open the file, otherwise interpret the argument as a string\n",
        "        try:\n",
        "            with open(sys.argv[2], \"r\") as f:\n",
        "                grammar_string = f.read()\n",
        "        except FileNotFoundError:\n",
        "            grammar_string = sys.argv[2]\n",
        "\n",
        "        (s, rules) = grammar()(grammar_string.strip())\n",
        "        if s:\n",
        "            print(\"Grammar had unparsed trailing characters:\\n\" + s)\n",
        "            sys.exit(1)\n",
        "        rules_by_name = { rule.rule_name : rule for rule in rules }\n",
        "        main_rule = rules[-1]\n",
        "        print(Grammar(rules_by_name, main_rule).grammar())\n",
        "\n",
        "    elif sys.argv[1] == \"--json\":\n",
        "        json_string = None\n",
        "        # try to open the file, otherwise interpret the argument as a string\n",
        "        try:\n",
        "            with open(sys.argv[2], \"r\") as f:\n",
        "                json_string = f.read()\n",
        "        except FileNotFoundError:\n",
        "            json_string = sys.argv[2]\n",
        "\n",
        "        print(json_spec(json_string).grammar())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "goPNm-nn9iKW",
        "outputId": "aa805ece-cad7-4de1-ecbc-bea228ab8185",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing grammar.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python grammar.py --json grammar_json.txt > compiled_grammar"
      ],
      "metadata": {
        "id": "4c2sVmT_9ren"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create FastAPI App\n",
        "This provides an API to the Llama 2 model. The model version can be changed in the code below as desired.\n",
        "\n",
        "For this demo we will use the 13 billion parameter version which is finetuned for instruction (chat) following.\n",
        "\n",
        "Despite the compression, it is still a more powerful model than the 7B variant."
      ],
      "metadata": {
        "id": "vUqG0_-6IChq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from typing import Any\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import LlamaGrammar, Llama\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GGML model required to fit Llama2-13B on a T4 GPU\n",
        "GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "GENERATIVE_AI_MODEL_FILE = \"llama-2-13b-chat.Q5_0.gguf\"\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=GENERATIVE_AI_MODEL_REPO,\n",
        "    filename=GENERATIVE_AI_MODEL_FILE\n",
        ")\n",
        "\n",
        "llama_grammar = LlamaGrammar.from_file(\"./compiled_grammar\")\n",
        "llama2_model = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=64,\n",
        "    n_ctx=2000\n",
        ")\n",
        "\n",
        "# Test an inference\n",
        "print(llama2_model(prompt=\"Hello \", max_tokens=1))\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# This defines the data json format expected for the endpoint, change as needed\n",
        "class TextInput(BaseModel):\n",
        "    inputs: str\n",
        "    parameters: dict[str, Any] | None\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def status_gpu_check() -> dict[str, str]:\n",
        "    gpu_msg = \"Available\" if tf.test.is_gpu_available() else \"Unavailable\"\n",
        "    return {\n",
        "        \"status\": \"I am ALIVE!\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(data: TextInput) -> dict[str, str]:\n",
        "    try:\n",
        "        params = data.parameters or {}\n",
        "        grammar = data.grammar_enabled\n",
        "        if grammar:\n",
        "          response = llama2_model(prompt=data.inputs, grammar=llama_grammar, **params)\n",
        "        else:\n",
        "          response = llama2_model(prompt=data.inputs, **params)\n",
        "        model_out = response['choices'][0]['text']\n",
        "        return {\"generated_text\": model_out}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ],
      "metadata": {
        "id": "Z6F078OHVssf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b39d9c2-d0d6-476d-be92-a8466bc54465"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start FastAPI Server\n",
        "The initial run will take a long time due to having to download the model and load it onto GPU.\n",
        "\n",
        "Note: interrupting the Google Colab runtime will send a SIGINT and stop the server."
      ],
      "metadata": {
        "id": "PikyQUQKIewj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell finishes quickly because it just needs to start up the server\n",
        "# The server will start the model download and will take a while to start up\n",
        "# ~5 minutes\n",
        "!uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &"
      ],
      "metadata": {
        "id": "HErEYVtPcGg9"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the logs at server.log to see progress.\n",
        "\n",
        "Wait until model is loaded and check with the next cell before moving on."
      ],
      "metadata": {
        "id": "tunHZmTHX98z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you see \"Failed to connect\", it's because the server is still starting up\n",
        "# Wait for the model to be downloaded and the server to fully start\n",
        "# Check the server.log file to see the status\n",
        "!curl localhost:8000"
      ],
      "metadata": {
        "id": "pJUJ4vICfQ7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd72223-210b-45ad-bd9a-de7f55400e32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to localhost port 8000 after 0 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Ngrok to create a public URL for the FastAPI server.\n",
        "**IMPORTANT:** If you created an account via email, please verify your email or the next 2 cells won't work.\n",
        "\n",
        "If you signed up via Google or GitHub account, you're good to go."
      ],
      "metadata": {
        "id": "csO2FA7LKWXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('./ngrok http 8000 &')"
      ],
      "metadata": {
        "id": "c-FpVEkifNdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the URL generated by the next cell, it should report that the FastAPI server is alive and that GPU is available.\n",
        "\n",
        "To hit the model endpoint, simply add `/generate` to the URL"
      ],
      "metadata": {
        "id": "XxzFtg3_Kfot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the Public URL\n",
        "# If this doesn't work, make sure you verified your email\n",
        "# Then run the previous code cell and this one again\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "metadata": {
        "id": "aCAIkVuxc3JU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6a4323-6430-4d67-fbcb-c834b4029609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http://0428-34-143-139-111.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shutting Down\n",
        "To shut down the processes, run the following commands in a new cell:\n",
        "```\n",
        "!pkill uvicorn\n",
        "!pkill ngrok\n",
        "```"
      ],
      "metadata": {
        "id": "liqVEsGfZPse"
      }
    }
  ]
}